# Correlación

```{r}
#| include: false

source("_common.R")
library(tidyverse)
library(ggthemes)
library(mvtnorm)

set.seed(1)
correlation <- c(-0.9999, -0.9, -0.75,-.5, -0.3, 0, 0.3, 0.75, 0.9, 0.9999)
n_sim <- 100
values <- NULL
for(i in seq_len(length(correlation))){
  rho <- correlation[i]
  sigma <- matrix(c(5, rho * sqrt(50), rho * sqrt(50), 10), 2, 2)
  sim <- rmvnorm(
    n = n_sim,
    mean = c(20,40),
    sigma = sigma
    ) %>%
    as_tibble() %>%
    mutate(correlation = round(rho,2))
  values <- bind_rows(values, sim)
}
names(values) <- c("x","y","correlation")
```

La correlación es el área de las estadísticas que estudia la relación sistemática entre dos o más variables e intenta contestar a preguntas como: ¿Si sube A va a subir B también? En este capítulo desarrollaremos algunas técnicas para contestar este tipo de pregunta.

## Visualización

El primer paso para estudiar posibles relaciones entre variables es visualizarlos. Si tenemos dos variables medidas por cada miembro de la población o muestra que estamos investigando podemos generar un *diagrama de dispersión* también conocido como *scatterplot*. En este tipo de visualización cada miembro de la muestra/población está representado por un punto, y las coordinadas del punto corresponde a las dos variables que hemos medido, en el eje horizontal y vertical respectivamente.

El la [figura @fig-strong-positive-correlation], vemos que la concentración de puntos suben de la izquierda a la derecha. Es decir cuando avanzamos en el eje horizontal avanzamos en el eje vertical también. Es un ejemplo de una *correlación positiva*, como podría ser edad y estatura.

```{r}
#| label: fig-strong-positive-correlation
#| echo: false
#| fig-cap: "Correlación positiva"
theme_set(theme_minimal()+
            theme(
              axis.text.x = element_blank(),
              axis.text.y = element_blank(),
              axis.ticks = element_blank(),
            
            )
)

#theme_set(theme_tufte())
  
  ggplot(data = values %>% filter(correlation ==.9), mapping = aes(x,y)) +
  geom_point() 

```

En la [figura @fig-strong-negative-correlation] vemos lo contrario, mientras avanzamos en el eje vertical retrocedemos (o bajamos) en el eje horizontal. Esto se conoce como correlación *negativa*.

```{r}
#| label: fig-strong-negative-correlation
#| echo: false
#| fig-cap: "Correlación negativa"

ggplot(data = values %>% filter(correlation ==-0.9), mapping = aes(x,y)) +
  geom_point() 

```

En la [figura @fig-weak-negative-correlation], también vemos correlación negativa, pero es menos fuerte que en la [figura @fig-strong-negative-correlation].

```{r}
#| label: fig-weak-negative-correlation
#| echo: false
#| fig-cap: "Correlación negative leve"
ggplot(data = values %>% filter(correlation ==-.75), mapping = aes(x,y)) +
  geom_point()
```

En la [figura @fig-near-perfect-negative-correlation] vemos una correlación negativa casi perfecta entre las dos variables.

```{r}
#| label: fig-near-perfect-negative-correlation
#| echo: false
#| fig-cap: "Correlación casi perfecta"
ggplot(data = values %>% filter(correlation < -.99), mapping = aes(x,y)) +
  geom_point()
```

En la [figura @fig-no-correlation] vemos un caso de correlación inexistente entre las variables en cuestión.

```{r}
#| label: fig-no-correlation
#| echo: false
#| fig-cap: "Correlación nula"
ggplot(data = values %>% filter(correlation ==0), mapping = aes(x,y)) +
  geom_point() 
```

En la [figura @fig-non-linear-relationship] vemos que existe una relación entre las dos variables, pero que esta no es lineal.[^10-correlaciones-1]

[^10-correlaciones-1]: De hecho es cuadrática: $y~\sim~x^2$.

```{r}
#| label: fig-non-linear-relationship
#| echo: false
#| fig-cap: "Relación no lineal"
set.seed(1)
tmpDF <- data.frame(x=seq(-7,7,by=.1))
tmpDF$y=tmpDF$x^2
for(i in 1:5){
tmpDF <- tmpDF %>% 
  bind_rows(data.frame(x=tmpDF$x,y=tmpDF$y+i))
}

tmpDF %>%
  sample_frac(.10) %>% 
  ggplot( aes(x,y))+
  geom_point()+ylab("y")

```

Las figuras [-@fig-strong-positive-correlation], [-@fig-strong-negative-correlation], [-@fig-weak-negative-correlation], [-@fig-near-perfect-negative-correlation], -@fig-no-correlation\] y [-@fig-non-linear-relationship] demuestran por qué es preciso graficar los datos al inicio del análisis. Nos da una indicación de si existe una correlación o no, si es positiva o negativa y que tan fuerte es. También nos podemos darnos cuenta de patrones en los datos que no son lineales, como es el caso de los datos en la [figura @fig-non-linear-relationship]. Asimismo, a veces nos encontramos con una correlación como la que vemos en la [figura @fig-near-perfect-negative-correlation]. Las correlaciones que son demasiado perfectas suelen ser un signo de advertencia y podemos preguntarnos si en realidad son dos variables distintas o si las dos están midiendo lo mismo.

```{r}
#| include: false
set.seed(1)
```

::: {#exm-example-scatterplot-i}
## Generar diagrama de dispersión en R

```{r}
# Generamos datos
datos = data.frame(
  x=rnorm(100),
  y=rnorm(100)
)

# Graficamos
plot(datos)
```
:::

En el [ejemplo @exm-example-scatterplot-i] utilizamos la función `rnorm` para generar cien observaciones aleatorias con distribución normal y los ponemos dentro de un `data.frame`. Luego usamos la función `plot` para graficarlos. Como nuestro `data.frame` tiene solo dos columnas R entiende que estos son los datos que queremos graficar. Si el `data.frame` tiene más columnas, podemos especificar los que queremos graficar así:

```{r}
#| eval: false
plot(datos$x,datos$y)
```

::: {#exm-example-scatterplot-ii
## Generar diagrama de dispersión en R

\
Por defecto R viene con algunos data.frames ya cargados, uno de ellos es «trees», podemos usar la función `head` para ver las primeras seis filas.

```{r}
head(trees)
```

Vemos que tiene tres columnas «Girth», «Height» y «Volume» (circunferencia, alto y volumen), los que, por lógica, deben tener alta correlación. Graficamos dos de ellos.

```{r}
plot(trees$Girth, trees$Volume)
```

Si usamos la función `plot` sin especificar columnas R entiende que queremos ver todas las combinaciones.

```{r}
plot(trees)
```

Este tipo de visualización puede ser útil cuando tenemos algunas variables y queremos darnos cuenta qué correlaciones hay. La visualización funciona bien hasta cierto número de columnas --ocho más o menos--, luego se vuelve difícil de leer y por ende de interpretar.
:::

## Coeficientes de correlación

Para tener una medida cuantitativa precisa de la correlación entre las variables calculamos un coeficiente de correlación. A continuación vamos a tres de ellos, el de Pearson, el de Spearman y el coeficiente $\phi$ (de la letra griega que corresponde a *f* en minúscula -- se pronuncia «fi». Los coeficientes de correlación se expresan por un número con varios decimales entre -1 y 1, donde -1 y 1 indican correlaciones perfectas, negativas y positivas respectivamente y 0 indica correlación nula.

El coeficiente Pearson es adecuado para datos de escala de razón o intervalo, el de Spearman para datos de escala ordinal y el coeficiente $\phi$ se usa para datos nominales.

### Coeficiente Pearson

Como ya mencionamos, el coeficiente de Pearson es apropiado cuando las variables a comparar con de escala de intervalo o razón ya que toma en cuenta la magnitud relativa de las observaciones.

Si tenemos un conjunto de pares de observaciones podemos representar el primer elemento del par por *x* y el segundo por *y*. Entonces el conjunto de los x van a tener una desviación estándar se calcula según la [definición @def-definition-standard-deviation], así:

$$
s_x = {\sqrt{(\sum(x-\bar{x})^2\over{N-1}}}.
$$

De la misma manera *y* tiene su desviación estándar: $$
s_y = {\sqrt{(\sum(y-\bar{y})^2\over{N-1}}}.
$$

Ahora podemos normalizar las variables según la [definición @def-definition-z-score] así:

$$
z_x = {x-\bar{x}\over{s_x}},
$$

$$
z_y = {y-\bar{y}\over{s_y}}.
$$

Y con estos datos podemos calcular el coeficiente según la [definición @def-definition-pearson-correlation].

::: {#def-definition-pearson-correlation}
## Coeficiente de correlación de Pearson

$$
r={\sum{z_xz_y}\over{N-1}}
$$ donde:

-   $\sum{z_xz_y}$: La suma de los productos[^10-correlaciones-2] de las dos variables normalizadas.
:::

[^10-correlaciones-2]: «Producto» en matemática es el resultado de una operación de multiplicación. Si multiplicamos $2\times2=4$, 4 es el producto.

Existe otra definición es matemáticamente equivalente y que se usa a veces para hacer el cálculo a mano:

::: {def-definition-pearson-correlation-ii
## Coeficiente de correlación Pearson

$$
r={N\Sigma{xy}-\Sigma{x}\Sigma{y}\over{\sqrt{\{N\Sigma{x^2}-(\Sigma{x})^2\}\times\{N\Sigma{y^2}-(\Sigma{y})^2\} }}}
$$
:::

::: {#exm-pearson-coefficient-calculation}
## Cálculo del Coeficiente de correlación de Pearson

\

```{r}
#| include: false
datos<- data.frame(
  Estudiante = c( 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12) %>% as.character(),
  x = c( 17, 13, 12, 14, 15, 8, 9, 13, 11, 14, 12, 16),
  y = c( 15, 13, 8, 17, 16, 9, 14, 10, 16, 13, 14, 17)
) 
```

En este ejemplo, adaptado de [@butler1985statistics], vamos a suponer que hemos tomado un examen de traducción y otro de comprensión de inglés a doce estudiantes. Los resultados de estos exámenes están en la [tabla @tbl-resultados-dos-examenes].

```{r}
#| label: tbl-resultados-dos-examenes
#| tbl-cap: "Resultados de un examen de tradución (x) y de comprensión (y) de ingles."
#| echo: false
datos %>% 
  knitr::kable(
    format.args = list(decimal.mark = ","),
                       booktabs=TRUE
  )
```

Para poder aplicar la fórmula vamos a precisar los valores llevados al cuadrado, el producto de $x\times y$ y las sumas de las columnas. Calculándolos obtenemos los datos de la [tabla @tbl-resultados-dos-examenes-ii].

```{r}
#| label: tbl-resultados-dos-examenes-ii
#| tbl-cap: "Resultados de un examen de tradución (x) y de comprensión (y) de ingles."
#| echo: false
datos <- datos %>% 
  mutate(x2=x^2,y2=y^2,xy = x*y) 
datos %>% 
  knitr::kable(
    format.args = list(decimal.mark = ","),
                       booktabs=TRUE
  )
```

```{r}
#| include: false
colSums(datos[2:6])

```

Sabemos que N=12, pero vamos a precisar las sumas de algunas columnas:

-   $\Sigma{x}$ = 154
-   $\Sigma{y}$ = 162
-   $\Sigma{x^2}$ = 2054
-   $\Sigma{y^2}$ = 2290
-   $\Sigma{xy}$ = 2124.

Aplicamos la fórmula:

$$
\begin{split}
&r= {N\Sigma{xy}-\Sigma{x}\Sigma{y}\over{\sqrt{\{N\Sigma{x^2}-(\Sigma{x})^2\}\times\{N\Sigma{y^2}-(\Sigma{y})^2\} }}} \\
&~~~~~~~\Updownarrow\\
&r={12\times2~124-154\times162\over{\sqrt{\{12\times2~054-154^2\}\times\{12\times2~290-162^2\} }}} \\
&~~~~~~~\Updownarrow\\
&r={25~488-24~948\over{\sqrt{\{24~648-23~716\}\times\{27~480-26~244\} }}} \\
&~~~~~~~\Updownarrow\\
&r= {540\over{\sqrt{932\times1236}}} \\
&~~~~~~~\Updownarrow\\
&r= {540\over{\sqrt{1~151~932}}} \\
&~~~~~~~\Updownarrow\\
&r= {540\over1~073,28} \\
&~~~~~~~\Updownarrow\\
&r= 0,5031
\end{split}
$$
:::

::: {#exm-pearson-coefficient-calculation-in-r}
## Cálculo del Coeficiente de correlación de Pearson en R

Si no queremos hacer todos los cálculos del [ejemplo @exm-pearson-coefficient-calculation] a mano podemos recurrir a R que con la función `cor` lo calcula.

```{r}
# Cargamos datos
datos<- data.frame(
  Estudiante = c( 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12),
  x = c( 17, 13, 12, 14, 15, 8, 9, 13, 11, 14, 12, 16),
  y = c( 15, 13, 8, 17, 16, 9, 14, 10, 16, 13, 14, 17)
) 

# Llamamos función
cor(datos$x, datos$y)
```
:::

### Coeficiente Spearman

Si una o ambas variables que estamos comparando son de escala ordinal, el coeficiente apropiado es el de Spearman. Para calcularlo ordenamos las observaciones de la primer variable de manera ascendiente y les damos el valor de su orden. Si dos observaciones de la misma variable tienen el mismo valor, si hay empates, se saca el promedio cual si el empate no hubiera existido. Hacemos lo mismo para la segunda variable. Calculamos la diferencia entre los rangos para cada par de observaciones. La correlación Spearman o $\rho$ de la letra griega *r* se calcula según la [definición @def-definition-spearman-correlation].

::: {#def-definition-spearman-correlation}
## Coeficiente de correlación de Spearman

$$
\rho = 1-{6\sum{d^2}\over{N(N^2-1)}}  
$$
:::

Imaginamos que pedimos a diez personas que ranqueen en una escala de uno a diez cuánto les gustaron dos cafeterías de Buenos Aires. Ya que los datos son de escala ordinal, tenemos que recurrir a *Spearman*. Usamos la misma función `cor` con un parámetro extra: `method = "spearman"` para indicar que queremos usar la correlación de Spearman.

```{r}
# Cargamos datos
rankings <- data.frame(
  Cafe.A = c(7, 6, 4, 5, 8, 7, 10, 3, 9, 2),
  Cafe.B = c(5, 4, 5, 6, 10, 7, 9, 2, 8, 1)
)

# Llamamos función
cor(rankings$Cafe.A, rankings$Cafe.B, method = "spearman")
```

Observamos que hay alto grado de correlación entre los ranking de los dos cafés.

### Coeficiente $\phi$

Si las dos variables en cuestión son nominales la pregunta se reduce a: ¿Si observamos la propiedad A es probable que observemos también B? Si estamos trabajando con datos educativos la pregunta podría ser ¿Si el estudiante responde correctamente el 1^r^ ítem, es probable que también acierte el 2^o^? Esto se puede representar en una tabla $2\times2$ como la que vemos en la [figura @fig-two-by-two-general].

```{r}
#| label: fig-two-by-two-general
#| echo: false
#| message: false
#| warning: false
#| fig-width: 5
#| fig-height: 2.5
#| fig-cap: "Tabla de contingencia dos por dos"
theme_set(theme_void())
g <- ggplot()
letter.size = 8
g <- g+
  annotate("text", x=1, y=5.6,    label="Variable 2", size=6, fontface=3)+
  annotate("text", x=3.25, y=5.6, label="Variable 1", size=6, fontface=3)+
  annotate("text", x=2.5,y=5,label="-", size=letter.size)+
  annotate("text", x=4,y=5,label="+", size=letter.size)+
  annotate("text", x=1.1,y=4,label="+", size=letter.size)+
  annotate("text", x=1.1,y=3,label="-", size=letter.size)+
  annotate("text", x=2.5,y=4, label="A", size=letter.size)+
  annotate("text", x=2.5,y=3, label="C", size=letter.size)+
  annotate("text", x=4,y=4, label="B", size=letter.size)+
  annotate("text", x=4,y=3, label="D", size=letter.size)+
  annotate("segment", x=.5,xend=4.5,y=4.5,yend=4.5)+
  annotate("segment", x=2,xend=2, y=2,yend=6)+
  xlim(0.5,5)
g
```

En esta tabla las celdas A, B, C y D son las frecuencias de las observaciones. Por ejemplo A sería el número de estudiantes que acertaron el 1^o^ pero no el 2^o^. B la frecuencia de estudiantes que pasaron ambos ítems y así sucesivamente. La correlación entre las dos variables se puede medir aplicando la formula de la [definición @def-phi-correlation].

::: {#def-phi-correlation}
## Coeficiente de $\phi$

$$
\phi = {{BC-AD}\over{\sqrt{(A+B)\times(C+D)\times(A+C)\times(B+D)}}}
$$
:::

Debería quedar claro que el coeficiente $\phi$ está estrechamente relacionado con la prueba de $\chi^2$ que vimos en la [definición @def-chi-squared]. De hecho se relacionan matemáticamente:

$$
\phi = \sqrt{\chi^2/N} \Leftrightarrow \chi^2 = N\times\phi^2.
$$ Por tanto la significanza de $\phi$ se puede obtener por medio de la conversión a $\chi^2$.

```{r, include=FALSE}
# Agregar ejemplo en R?
```

## Interpretación de correlaciones

Es muy importante entender que una correlación, incluso alta, entre dos variables no quiere decir que la relación entre ellas es de causa y efecto. Si hacemos una muestra en la escuela secundaria y medimos estatura, por un lado, y nivel de inglés por otra, es bastante probable que encontremos una correlación muy fuerte entre las dos variables. Pero debería estar claro que ni la estatura causa conocimientos de inglés ni tampoco lo contrario. Lo que claramente pasa es que mas *edad* los estudiantes tienen más estatura y han cursado más niveles de inglés.

Saltar a conclusiones sobre causalidad basadas en correlaciones es tal vez el error estadístico más frecuente en la literatura tanto académica como periodística. Nunca hay que olvidar que una correlación significativa solo nos dice que existe una relación *matemática* entre dos variables. No nos indica cómo interpretarla ni mucho menos sobre sus causas y efectos.

## Glosario

```{r}
#| echo: false
#| results: asis

.format_dataframe_as_def_list(chapter = 10)

```
